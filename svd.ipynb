{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "francois.role@parisdescartes.fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let $X$ be a N * M document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X=np.array([ [1., 1., 1., 0., 0.] ,\n",
    "             [2., 2., 2., 0. ,0.],\n",
    "             [1., 1. ,1., 0., 0.] ,\n",
    "             [5., 5. ,5. ,0., 0.] ,\n",
    "             [0., 0. ,0., 2., 2.] ,\n",
    "             [0., 0., 0. ,3. ,3.] ,\n",
    "             [0., 0., 0. ,1.,1.]\n",
    "              ])\n",
    "# cols = car vehicle truck cat dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.matrix_rank(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the eigenvectors of $XX^t$ and use them as columns of the $U$ matrix (documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1796053   0.        ]\n",
      " [ 0.3592106   0.        ]\n",
      " [ 0.1796053   0.        ]\n",
      " [ 0.89802651  0.        ]\n",
      " [ 0.         -0.53452248]\n",
      " [ 0.         -0.80178373]\n",
      " [ 0.         -0.26726124]]\n",
      "[ 9.64365076+0.j  5.29150262+0.j]\n"
     ]
    }
   ],
   "source": [
    "XXt=X.dot(X.T)\n",
    "eigVals, eigVects= linalg.eig(XXt)\n",
    "eigValsIndices=eigVals.argsort()\n",
    "eigValsIndices=eigValsIndices[:-3:-1] \n",
    "topEigVects=eigVects[:, eigValsIndices]\n",
    "\n",
    "print (topEigVects)\n",
    "print\n",
    "print (np.sqrt(eigVals[eigValsIndices]))  # singular values\n",
    "U=topEigVects\n",
    "S=np.diag(np.sqrt(eigVals[eigValsIndices]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the eigenvectors of $X^tX$ and use them as columns of the $V$ matrix (terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.57735027  0.        ]\n",
      " [ 0.57735027  0.        ]\n",
      " [ 0.57735027  0.        ]\n",
      " [ 0.          0.70710678]\n",
      " [ 0.          0.70710678]]\n",
      "[ 9.64365076+0.j  5.29150262+0.j]\n"
     ]
    }
   ],
   "source": [
    "XtX=X.T.dot(X)\n",
    "eigVals, eigVects= linalg.eig(XtX)\n",
    "eigValsIndices=eigVals.argsort()\n",
    "eigValsIndices=eigValsIndices[:-3:-1] \n",
    "topEigVects=eigVects[:, eigValsIndices]\n",
    "\n",
    "print (topEigVects)\n",
    "print\n",
    "print (np.sqrt(eigVals[eigValsIndices]) ) # singular values\n",
    "V=topEigVects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruct the matrix. We can do it exactly. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.+0.j  1.+0.j  1.+0.j  0.+0.j  0.+0.j]\n",
      " [ 2.+0.j  2.+0.j  2.+0.j  0.+0.j  0.+0.j]\n",
      " [ 1.+0.j  1.+0.j  1.+0.j  0.+0.j  0.+0.j]\n",
      " [ 5.+0.j  5.+0.j  5.+0.j  0.+0.j  0.+0.j]\n",
      " [ 0.+0.j  0.+0.j  0.+0.j -2.+0.j -2.+0.j]\n",
      " [ 0.+0.j  0.+0.j  0.+0.j -3.+0.j -3.+0.j]\n",
      " [ 0.+0.j  0.+0.j  0.+0.j -1.+0.j -1.+0.j]]\n",
      "\n",
      "[[ 1.  1.  1.  0.  0.]\n",
      " [ 2.  2.  2.  0.  0.]\n",
      " [ 1.  1.  1.  0.  0.]\n",
      " [ 5.  5.  5.  0.  0.]\n",
      " [ 0.  0.  0.  2.  2.]\n",
      " [ 0.  0.  0.  3.  3.]\n",
      " [ 0.  0.  0.  1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "print(U.dot(S).dot(V.T))\n",
    "\n",
    "print()\n",
    "\n",
    "print(X)\n",
    "#U.shape\n",
    "#S.shape\n",
    "#V.T.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the distance between the original and reconstructed matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8709810974240868e-15"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstructed=np.abs(U.dot(S).dot(V.T))\n",
    "\n",
    "np.sqrt(np.sum ( (X - reconstructed)**2  ) )  # Frobenius"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first document is represented in the latent space by the first row of matrix $U$ :\n",
    "   \n",
    "[ 0.1796053 ,   0.  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The relation between the original representation of the documents and their representation in the latent semantic space can be derived as follows:\n",
    "$$\n",
    "X=U \\Sigma V^T   \\Leftrightarrow X V {\\Sigma}^{-1} = U\n",
    "$$\n",
    "\n",
    "Hence the \"conceptual\" representation of the documents (the $U$ matrix) can be obtained by projecting the original representation of the documents into the term latent space $V$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.1796053 ,  0.        ],\n",
       "        [ 0.3592106 ,  0.        ],\n",
       "        [ 0.1796053 ,  0.        ],\n",
       "        [ 0.89802651,  0.        ],\n",
       "        [ 0.        ,  0.53452248],\n",
       "        [ 0.        ,  0.80178373],\n",
       "        [ 0.        ,  0.26726124]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_inv=np.mat(S).I\n",
    "np.abs(X.dot(V).dot(S_inv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In the same spirit, we can derive $V$ from $U$ and $X$:\n",
    "$$\n",
    "X=U \\Sigma V^T   \\Leftrightarrow V= X^{-1}U{\\Sigma}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[  5.77350269e-01,   0.00000000e+00],\n",
       "        [  5.77350269e-01,   0.00000000e+00],\n",
       "        [  5.77350269e-01,   0.00000000e+00],\n",
       "        [  6.66333430e-34,   7.07106781e-01],\n",
       "        [  7.49540088e-34,   7.07106781e-01]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_inv=np.mat(X).I\n",
    "np.abs(X_inv.dot(U).dot(S))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application to Information Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This leads to useful applications. For example, in the Information Retrieval area, suppose we are given a query $q$. We can treat $q$ as a short document and project it into the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X=np.mat([   [1., 1., 0., 0., 0.] ,\n",
    "             [0., 0., 2., 0. ,0.],\n",
    "             [1., 0. ,1., 0., 0.] ,\n",
    "             [0., 0., 5. ,0., 0.] ,\n",
    "             [0., 0. ,0., 2., 2.] ,\n",
    "             [0., 0., 0. ,3. ,3.] ,\n",
    "             [0., 0., 0. ,1.,1.]\n",
    "              ])\n",
    "# cols = car vehicle truck cat dog\n",
    "\n",
    "q=np.mat([1., 1., 0., 0., 0.]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a standard query model, we would miss the second and fourth document altough they are related to the automotive sector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 2.],\n",
       "        [ 0.],\n",
       "        [ 1.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X * q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution is to project $q$ into the latent space. If $q$ is represented as a row vector, the projected query $\\hat{q}$ will be obtained as:\n",
    "\n",
    "$$\n",
    "\\hat{q}= q V {\\Sigma}^{-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00673646  0.        ]\n",
      " [ 0.36469846  0.        ]\n",
      " [ 0.1888614   0.        ]\n",
      " [ 0.91174614  0.        ]\n",
      " [ 0.         -0.53452248]\n",
      " [ 0.         -0.80178373]\n",
      " [ 0.         -0.26726124]]\n",
      "====\n",
      "[[ 5.48048471+0.j  0.00000000+0.j]\n",
      " [ 0.00000000+0.j  5.29150262+0.j]]\n",
      "====\n",
      "[[-0.03568988  0.        ]\n",
      " [-0.00122917  0.        ]\n",
      " [-0.99936216  0.        ]\n",
      " [ 0.          0.70710678]\n",
      " [ 0.          0.70710678]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "XXt=X.dot(X.T)\n",
    "eigVals, eigVects= linalg.eig(XXt)\n",
    "eigValsIndices=eigVals.argsort()\n",
    "eigValsIndices=eigValsIndices[:-3:-1] \n",
    "U=eigVects[:, eigValsIndices]\n",
    "\n",
    "S=np.mat(np.diag(np.sqrt(eigVals[eigValsIndices])))\n",
    "\n",
    "XtX=X.T.dot(X)\n",
    "eigVals, eigVects= linalg.eig(XtX)\n",
    "eigValsIndices=eigVals.argsort()\n",
    "eigValsIndices=eigValsIndices[:-3:-1] \n",
    "V=eigVects[:, eigValsIndices]\n",
    "\n",
    "print(U)\n",
    "print(\"====\")\n",
    "print(S)\n",
    "print(\"====\")\n",
    "print(V)\n",
    "\n",
    "\n",
    "S_inv=S.I\n",
    "q_hat= q.T * V * S_inv  # q.T = a row like a doc in a doc-term matrix\n",
    "\n",
    "q_hat= np.abs(np.array(q_hat).flatten() )\n",
    "U.dot(q_hat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to match documents (second and fourth) that were about automotive area but had no terms in common with our query!\n",
    "\n",
    "\n",
    "********************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If we project the conceptual representation of the 4th doc onto V, we get back the original representation of the 4th doc :\n",
    "$$\n",
    "X_{j.}= \\hat{V} \\hat{\\Sigma } {\\hat{U}_{j.}}^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[-0.17833560+0.j, -0.00614194+0.j, -4.99362361+0.j,  0.00000000+0.j,\n",
       "          0.00000000+0.j]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V.dot(S).dot(U[3,:].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We now know how to project a document into the k-dimensional latent space:\n",
    "$$\n",
    "{\\hat{U}_{j.}}^T= ({\\hat{\\Sigma }}^{-1} {\\hat{V}}^T) X_{j.}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify that we get the same result as that obtained from Scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import linalg\n",
    "U , S , V= linalg.svd(X)\n",
    "print (\"U\")\n",
    "print( np.c_[  np.mat(U[:,0]).T , np.mat(U[:,1]).T ])\n",
    "print \n",
    "print (\"S\")\n",
    "print (S)\n",
    "print\n",
    "print (\"V^t\")\n",
    "print ( np.r_[  np.mat(V[0,:]) , np.mat(V[1,:]) ])\n",
    "U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "S=np.mat(S)\n",
    "S.I.shape\n",
    "V.T\n",
    "U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To do Frobenius norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project a document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project the documents onto the eigenvectors of $X^tX$ ($V$, \"term to concept\" matrix\").\n",
    "Document $4$ has an \"affinity\" of $8.6$ with the \"first concept\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eigVals, eigVects= linalg.eig(XtX)\n",
    "eigValsIndices=eigVals.argsort()\n",
    "eigValsIndices=eigValsIndices[:-3:-1] \n",
    "topEigVects=eigVects[:, eigValsIndices]\n",
    "\n",
    "print (X.dot(topEigVects))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project the terms onto the eigenvectors of $XX^t$ ($U$, \"document to concept\" matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eigVals, eigVects= linalg.eig(XXt)\n",
    "eigValsIndices=eigVals.argsort()\n",
    "eigValsIndices=eigValsIndices[:-3:-1] \n",
    "topEigVects=eigVects[:, eigValsIndices]\n",
    "\n",
    "print( X.T.dot(topEigVects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-f0dbf86bc46a>, line 21)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-13-f0dbf86bc46a>\"\u001b[1;36m, line \u001b[1;32m21\u001b[0m\n\u001b[1;33m    print np.linalg.norm(A_col_normalized, axis=0)\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Exemple Master 2 ///////////////////////////////////////////////////////////////\n",
    "\n",
    "A=np.array( [ [0,1,0,1,1,0,1],\n",
    "[0,1,1,0,0,0,0],\n",
    "[0,0,0,0,0,1,1],\n",
    "[0,0,0,1,0,0,0],\n",
    "[0,1,1,0,0,0,0],\n",
    "[1,0,0,1,0,0,0],\n",
    "[0,0,0,0,1,1,0],\n",
    "[0,0,1,1,0,0,0],\n",
    "[1,0,0,1,0,0,0 ] ], dtype=float)\n",
    "\n",
    "A_col_norms=np.linalg.norm(A, axis=0)\n",
    "A_col_norms_matrix = np.tile(A_col_norms, (9,1))\n",
    "A_col_normalized= A * (1. / A_col_norms_matrix)\n",
    "\n",
    "q = np.array([1, 0, 0, 1 ,0, 0, 0, 0, 0])\n",
    "q_norm=np.linalg.norm(q)\n",
    "q_normalized= q / q_norm\n",
    "\n",
    "print np.linalg.norm(A_col_normalized, axis=0)\n",
    "\n",
    "simScores= q_normalized.dot(A_col_normalized)\n",
    "print simScores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-14-2f8461f20076>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-14-2f8461f20076>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    print  U.shape, V.shape, s.shape\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# /////////  FULL SVD   //////\n",
    "\n",
    "a = A\n",
    "U, s, V = np.linalg.svd(a, full_matrices=True)\n",
    "print  U.shape, V.shape, s.shape\n",
    "S = np.zeros((9, 7), dtype=float)\n",
    "S[:7, :7] = np.diag(s)\n",
    "print\n",
    "print S\n",
    "print\n",
    "print a\n",
    "print\n",
    "print np.dot(U, np.dot(S, V))\n",
    "\n",
    "# ////////////  Frobenius Norm  ///////\n",
    "\n",
    "print np.linalg.norm(A -   np.dot(U, np.dot(S, V))  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-d7f7db9f243c>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-15-d7f7db9f243c>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    print U.shape\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# /////////////  RANK 4 approximation  //////////\n",
    "\n",
    "U, s, V = np.linalg.svd(A, full_matrices=True)\n",
    "\n",
    "print U.shape\n",
    "\n",
    "print s\n",
    "\n",
    "print V.shape\n",
    "\n",
    "U_4=U[:,:7]\n",
    "V_4=V[:7,:]\n",
    "S_4=np.diag(s[:7])\n",
    "\n",
    "print  U_4.shape, V_4.shape, S_4.shape\n",
    "\n",
    "print\n",
    "\n",
    "print A\n",
    "print\n",
    "A_4=np.dot(U_4, np.dot(S_4, V_4))\n",
    "print A_4 # or  U_4.dot(s_4).dot(V_4)\n",
    "\n",
    "# ////////////  Frobenius Norm  ///////\n",
    "\n",
    "print \"Frob. norm\" , np.linalg.norm(A - A_4)\n",
    "\n",
    "# ///////  Normalize the reduced A_4   /////////\n",
    "\n",
    "print np.linalg.norm(A_4 , axis=0)\n",
    "\n",
    "A4_col_norms=np.linalg.norm(A_4, axis=0)\n",
    "A4_col_norms_matrix = np.tile(A4_col_norms, (9,1))\n",
    "A4_col_normalized= A_4 * (1. / A4_col_norms_matrix)\n",
    "\n",
    "print np.linalg.norm(A4_col_normalized , axis=0)\n",
    "\n",
    "# /////  Query the reduced matrix //////\n",
    "\n",
    "q = np.array([1, 0, 0, 1 ,0, 0, 0, 0, 0])\n",
    "q_norm=np.linalg.norm(q)\n",
    "q_normalized= q / q_norm\n",
    "\n",
    "simScores= q_normalized.dot(A4_col_normalized)\n",
    "print simScores\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
